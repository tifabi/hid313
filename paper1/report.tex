\documentclass[sigconf]{acmart}

\input{format/i523}

\begin{document}
\title{Big Data Platforms as a Service}


\author{Tiffany Fabianac}
\orcid{}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{}
}
\email{tifabi@iu.edu}

\renewcommand{\shortauthors}{T. Fabianac}


\begin{abstract}
Big Data platform solutions allow data producers to use data to the fullest potential by combining processing engines with storage solutions and analytic technologies. Pharmaceutical clients are looking into platform solutions to safely store, analyze, and use clinical trial data, experimental data, drug development studies, drug production, regulation, and a number of other outlets. Just a few of the benefits of using a platform solution to manage these data outlets are possibly not having to change current work processes, that management and other research groups can access and use data without needing special access to systems, and scalability of storage and analytic components is seamless. The problems faced to implementing big data platform solutions include the selection of a platform vendor, the design of appropriate data architecture, and establishing effective user interfaces.
\end{abstract}

\keywords{i523, HID313, Big Data, Platform, Cloud Architecture}


\maketitle

\TODO{changed format}

\section{Introduction}
Most pharmaceutical companies have adopted one or many Laboratory Information Management Systems (LIMS) and/or Electronic Laboratory Notebooks (ELN). These systems are often implemented as standalone systems within a single Research and Development (R\&D) group or even within a single laboratory. A problem seen in large- or mid-sized pharmaceutical companies is that different research groups within the same organization often implement isolated LIMS or ELN. This severely restricts data sharing and reuse between groups which leads to many problems such as the same experiment being run multiple times between different groups, regulatory inefficiencies in tracking sample use and storage, and bottle necked development cycles due to missing data. 

One of the emerging strategies to combat the problems arising from isolated systems is to combine systems using cloud computing. Platform as a Service (PaaS) provides an environment for the development and execution of applications and software tools. The platform is the heart of a cloud computing infrastructure that enables the software on-top as well as any data created to be accessed and used by a multitude of users\cite{Ojala}.

\section{Importance of Platforms}	
Many organizations struggle to share data and processing tools among researchers. PaaS provides a method of better resource utilization while reducing maintenance costs\cite{Oh}. As pharmaceutical companies collect larger and larger masses of data through LIMS, ELN, and other systems the need for scalable storage becomes inescapable. Cloud storage available with the implementation of a PaaS solves current and predictable future data storage needs as clinical trial data becomes truly digital and full genome analysis becomes more available. The surge of stored data requires access to tools with the capability of pulling insights from the data. These analytic tools are available in familiar formats that statisticians know and love such as SAS, but new analytic tools have been built into platform environments as well as pushed the development of new market players like Tabeau and Spotfire\cite{Talia}.

A pharmaceutical company's R\&D group is made up of several diverse units such as analytical chemistry, oncology, genomics, etc. Each of these groups has their own set of unique requirements and thus require multiple solutions to be implemented across the R\&D organization. A problem arises now when an FDA regulator enters the lab space and requires an audit trail for a single sample. The sample was aliquoted and distributed across several groups and R\&D management needs to be able to prove to the FDA regulator that the sample has been used only for its designated purpose and has been properly destroyed. The sample's use is recorded in several different LIMS and an ELN which the R\&D manager does not have access to. With a properly implemented PaaS the manager can print the usage audit trail from each system without accessing them individually. The manager can pull destruction records and storage locations from current inventory and deliver these records to the FDA regulator without directly contacting any of the lab groups. 


\section{Implementing Platforms}
Implementing a platform raises a number of concerns around security, selecting the right solution, designing the data architecture and associated relationships, and planning the user interface. All of the large platform providers have invested enormous amounts of resources into assuring the security of their data storage solutions. The right solution might be based on available applications, the storage solution's design, the cost, the learning curve for use, or a number of other client based requirements. Data architecture has the overarching purpose to design the data warehouse solution without limitations to growth, analysis tools, or query speed. User interface depends mostly on the user requirements, it could be driven by how much visibility is needed and how read and write privileges are designated. 

The overarching concern with storing data outside of the organization is security. Numerous methods have been developed to assure cloud security such as integrated stacks used by Google and Microsoft Azure and Service Level Agreements (SLAs)\cite{Casola}. Cloud companies are required to maintain high security at all levels. Google runs various vulnerability reward programs that pay developers, hackers, and security experts for finding security bugs. In addition to the product bugs, Google also maintains high security at their data centers, which includes laser beam intrusion detection, multi-factor access control, and biometrics to a limited population of less than 1\% of Googlers\cite{www-gcp-security}. 

\section{Identifying the Right PaaS}
Every organization has a unique set of user requirements and every organization shares a certain number of user requirements. Something as simple as requiring a username and password to access content is a requirement shared across the great majority of systems while the need to create complex animal breeding plans that produce offspring with genetic content for 20 specific alleles may be a requirement for one unique client. A market analysis weighing a platform's capabilities against the organization's requirements will often help to narrow down this expanding market. Some of the largest PaaS providers are Microsoft, Amazon, and Google. 

Microsoft big data solutions have taken advantage of open source technologies by setting Hadoop as the center of their big data platform. Hadoop is implemented through Hortonworks Data Platform (HDP) which has been developed as an open source solution with Apache and other open source components. Microsoft allows cloud and on-premise implementation, but generally local environments are only used as proof of concept testing. Microsoft platform solutions allow for data to be manipulated and used in Microsoft tools such as Sharepoint and Excel while big data analysis, visualization, and mining can be performed using SQL Server Analysis Services or HDInsight. The Hadoop-based platform has no limitations with structured or unstructured data, a number of additional tools are available for data storage, and efficient queries provide a potential boost to discovery. Microsoft Azure storage runs \$40 a month per 1TB and employs a pay for use plan to resource use within the platform's toolbox\cite{www-msdn}.

Amazon Web Services (AWS) offers data storage solutions in NoSQL and Relational Database models. Interactions with these data engines can be done using Hadoop, Interactive Query Service, or Elasticsearch. Amazon has designed their storage sources in such a way that clients can use any preferred open source application, but Amazon has also developed a toolbox of analytic tools. Amazon offers data warehousing through Amazon Redshift, which allows for management, query, and analysis at the petabyte-scale. Amazon storage runs around \$80 a month per 1TB. AWS offers Business Intelligence, Artificial Intelligence, Machine Learning, Internet of Things, Serverless Computing, and a number of data interface tools available in a pay-as-you-use billing form\cite{www-aws}. 

Google Cloud Platform (GCP) offers a complete end-to-end data storage solution, which allows the use of GCP developed systems and open source tools. BigQuery is Google's data warehouse tool which is serverless and requires no infrastructure management with the assist of Google Cloud Dataflow. Dataflow eliminates the need for resource management and performance optimization. GCP storage runs \$10 a month per 1TB. GCP has a number of applications for data manipulation. Dataproc allows dataset management through Hadoop and Spark, data visualization can be generated through Datalab, Data Studio, and Dataprep which are all Google developed applications\cite{www-gcp}.

\section{Designing the Data Architecture}
All data storage solutions from relational databases to noSQL data stores to cloud data warehouses have to start with a defined architecture. The data architecture model will illustrate how data components will be organized and connected. The mindset of a data architect should be focused on reducing the complexity of a data model while maintaining the highest level on utilization. This can be a fine line to walk as a designer. Complexity can be reduced by breaking user requirements down to the most basic and generalized principles to define the simplest data modules. An example of this might be a system that requires a number of different requests and instead of designing a component for vendor requests, user requests, and management requests the component is designed for request and request type. This generality allows for easy future scaling or additional system requirements not yet defined. Cloud systems maintain high utilization by manipulating data using strategic layering. One layer for storage, one layer for defining storage keys, another for combining query tools, another for consolidating query results and so on. With the more established cloud offerings a lot of these layers have already been supplied, but the transitions and interconnections still have to be outlined by a designer\cite{Saltzer}.

\section{Designing the User Interface}
A system's user interface (UI) must be laid out in a simple and intuitive manner that allows users to perform the tasks required while exploring new insights provided by generated data. There are a number of influences leading to the development of user interfaces such as familiarity; users are familiar and comfortable performing a search in Google or Amazon interfaces and maintain the same high expectation with their working environment. If a user requires sample tracking or auditing, they may relate the need to how a package is tracked with FedEx or UPS and expect the same level of access and insight to sample tracking within their working environment. Users may even have an information management system that they use and are comfortable with so switching to a new UI can be daunting as it requires additional training and most likely new work processes.

UI developers have the challenging job of creating the face of an application. A poorly designed face may not attract as many customers as something with a higher graphical output. Even a strong performing system can be downgraded or completely ignored by users if its front end is poorly laid out. Considerations for a UI design include font-size, space between elements, interactive space, and line-width which can all differ across devices such as between a tablet, desktop, or smartphone\cite{Macik}. 

\section{Conclusion}
As more and more companies realize the value of their data, platforms and associated tools become more and more vital to organizational success. The pharmaceutical industry knows that data is king, but is experiencing major bottlenecks in deploying platform solutions for the reasons discussed: the cost and complexity of implementation, the concern over security, the frustration of changing or creating new work processes. Current information management systems help scientists and researchers work exponentially faster than they ever could on paper, but current systems are not designed to facilitate sharing of ideas. This is where platforms come in. A regulatory supervisor should not need training on every information management system to effectively regulate the use and disposal of clinical samples. A laboratory technician should not need to wait for specific system privileges to access a study that the organization did in a different lab space, whether it's in the same building or on the other side of the globe. Platform services are allowing scientists and managers to share ideas more efficiently than they ever have before and the pharmaceutical industry has the potential to exploit this new technology to improve life expectancy, make drugs safer, and research smarter. 

\begin{acks}

 Â The authors would like to thank Dr. Gregor Von Laszewski  and Teaching Assistants Hyungro Lee, Juliette Zerick, Saber Sheybani Moghadam, and Miao Jiang.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

\end{document}

